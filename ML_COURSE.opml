<?xml version="1.0" encoding="UTF-8"?>
<opml version="2.0">
  <head>
    <title></title>
    <dateModified></dateModified>
    <ownerName></ownerName>
  </head>
  <body>
<outline text="Supervised Learning">
</outline>
<outline text="Unsupervised Learning">
</outline>
<outline text="Python Libs(Packages)">
  <outline text="scikit-learn" _note="-   **Iris datasets**&#10;&#10;-   &#10;&#10;">
    <outline text="depends on NumPy and SciPy">
    </outline>
    <outline text="Numpy(key point)">
    </outline>
    <outline text="Scipy">
    </outline>
    <outline text="matplotlib(key point)">
    </outline>
    <outline text="pandas">
    </outline>
    <outline text="mglearn &lt;a href=&quot;https://github.com/amueller/introduction_to_ml_with_python&quot;&gt;本书github代码&lt;/a&gt;">
    </outline>
  </outline>
</outline>
<outline text="Chapter2 Supervised Learning 监督学习">
  <outline text="2.1 分类（classifification）和回归（regression）">
    <outline text="#分类" _note="-   分类问题的目标是**预测类别标签**（class&#10;    label），这些标签来自**预定义的可选列表**&#10;&#10;-   分类问题有时可分为**二分类** （binary&#10;    classifification，在两个类别之间进行区分的&#10;&#10;    一种特殊情况）和**多分类**（multiclass&#10;&#10;    classifification，在两个以上的类别之间进行区分）&#10;&#10;-   **二分类**（寻找垃圾邮件）中，通常一个类别称为**正类**（positive&#10;    class），另一个类别称为**反类**（negative class）&#10;&#10;-   **多分类**（鸢尾花，根据网站上的文本预测网站所用的语言。这里的类别就是预定义的语言表）&#10;&#10;">
    </outline>
    <outline text="#回归" _note="-   回归任务的目标是**预测一个连续值**，编程术语叫作**浮点数**（floating-point&#10;    number），数学术语叫作**实数**（real number）&#10;&#10;-   举例：&#10;&#10;    -   根据教育水平、年龄和居住地来预测一个人的**年收入**&#10;&#10;    -   根据上一年的产量、天气和农场员工数等属性来预测玉米农场的**产量**&#10;&#10;    -   预测值可以**在给定范围内任意取值**&#10;&#10;">
    </outline>
    <outline text="#区分分类和回归" _note="-   **输出是否具有某种连续性**（ some kind of ordering or continuity in&#10;    the output）&#10;&#10;">
    </outline>
  </outline>
  <outline text="2.2 泛化、过拟合与欠拟合" _note="-   如果一个模型能够对没见过的数据做出准确预测，我们就说它能够从**训练集泛化**（generalize）到**测试集**。我们想要构建一个泛化精度尽可能高的模型。&#10;&#10;-   **通常**来说，我们构建模型，使其在**训练集**上能够做出准确**预测**。如果训练集和测试集足够**相似**，我们预计模型在**测试集**上也能做出**准确预测**。&#10;&#10;-   判断一个算法在新数据上表现好坏的**唯一度量**，就是在**测试集**上的评估，测试集是用来测试模型**泛化性能**的数据&#10;&#10;-   **简单**的模型对新数据的**泛化**能力**更好**&#10;&#10;-   模型**过于复杂**，被称为**过拟合**（**overfitting**）,在拟合模型时**过分关注训练集的细节**，得到了一个在**训练集上表现很好**(预测效果好)、但不能泛化到新数据上的模型&#10;&#10;-   模型**过于简单**，模型甚至在训练集上的表现就很差，选择过于简单的模型被称为**欠拟合**（**underfitting**）&#10;&#10;-   二者之间存在一个**最佳位置**，可以得到最好的泛化性能，过拟合与欠拟合之间的权衡：&#10;&#10;    ![](F:\Typora_md\ML_COURSE\模型复杂度与训练精度和测试精度之间的权衡.png)&#10;&#10;">
    <outline text="模型复杂度与数据集大小的关系" _note="-   模型复杂度与训练数据集中输入的变化密切相关,数据集中包含的数据点的**变化范围越大**，在不发生过拟合的前提下你可以使用的**模型就越复杂**,收集**更多数据**，**适当构建**更复杂的模型，对监督学习任务往往特别有用&#10;&#10;">
    </outline>
  </outline>
  <outline text="2.3监督学习算法">
    <outline text="2.3.1 一些样本数据集" _note="使用低维数据集：（**特征较少，可视化简单**），结论不适用于高维数据集（特征较多）&#10;&#10;-   **模拟**的forge二分类数据集，绘制散点图，**第一个特征**为x轴，**第二个特征**为y轴&#10;&#10;    `mglearn.discrete_scatter(X[:, 0], X[:, 1], y)`&#10;&#10;    ``` {.python}&#10;    Out[2]:&#10;    X.shape: (26, 2) #这个数据集包含26个数据点和2个特征&#10;    ```&#10;&#10;-   **模拟**的wave数据集说明回归算法，wave 数据集只有&#10;&#10;    一个输入特征和一个连续的目标变量（或响应），后者是模型想要预测的对象。**单一特征**位于x轴，**回归目标**（输出）位于y轴。&#10;&#10;    现实数据集包含在scikit-learn中,通常被保存为Bunch对象,可以用点操作符来访问对象的值（比&#10;&#10;    如用**bunch.key**来代替bunch\['key'\]）。&#10;&#10;-   **现实**世界的cancer数据集，肿瘤（良性：benign,恶性：malignant）,其任务是基于人体组织的测量数据来**学习预测**肿瘤是否为**恶性**。&#10;&#10;    ``` {.python}&#10;    from sklearn.datasets import load_breast_cancer&#10;    cancer = load_breast_cancer()&#10;    ```&#10;&#10;    `cancer.data.shape: (569,30)`&#10;    569个数据点，每个数据点有30个特征，**具体详见书本介绍**。&#10;&#10;-   **现实**世界的**回归**数据集，即波士顿房价数据集。任务：利用犯罪率、是否邻近查尔斯河、公路可达性等信息，来预测20世纪70年代波士顿地区房屋价格的中位数。&#10;&#10;    ``` {.python}&#10;    from sklearn.datasets import load_bostonboston = load_boston()&#10;    ...&#10;    ```&#10;&#10;    `Data shape: (506, 13)`&#10;&#10;    我们不仅将犯罪率和公路可达性作为特征，还将&#10;&#10;    犯罪率和公路可达性的**乘积**作为特征。像这样包含导出特征的方法叫作**特征工程**（feature&#10;&#10;    engineering）,将在第4章中详细讲述。&#10;&#10;">
    </outline>
    <outline text="2.3.2 k近邻 k-Nearest Neighbor" _note="-   最简单的机器学习算法&#10;&#10;-   构建模型只需要保存**训练数据集**即可。想要&#10;&#10;    对新数据点做出**预测**，算法会在**训练数据集中**找到最近的数据点，也就是它的“**最近邻**”。&#10;&#10;    1.  **k近邻分类**&#10;&#10;        -   k-NN算法最简单的版本只考虑一个最近邻（**单一最近邻算法**）：对于**每个新数据点**，标记了**训练集中**与它**最近**的点。&#10;&#10;        -   还可以考虑任意个（**k个**）邻居，用“**投票法**”（voting）来指定标签。就是说，对&#10;&#10;            于**每个测试点**，数一数多少个邻居属于类别0，多少个邻居属于类别1。然后将出现&#10;&#10;            **次数更多的类别**（也就是k个近邻中占多数的类别）作为**预测结果**。（看图）同样适用于多分类数据集。&#10;&#10;            ``` {.python}&#10;            clf = KNeighborsClassifier(n_neighbors=3) #实例化类&#10;            clf.fit(X_train, y_train) #利用训练集对分类器进行拟合&#10;            print(&quot;Test set predictions: {}&quot;.format(clf.predict(X_test)))#调用predict方法来对测试数据进行预测&#10;            print(&quot;Test set accuracy: {:.2f}&quot;.format(clf.score(X_test, y_test)))#评估模型的泛化能力好坏，对测试数据和测试标签调用score方法&#10;            ```&#10;&#10;    2.  **分析KNeighborsClassifier**&#10;&#10;        根据平面中每个点所属的类别对平面进行着色。这样可以查看**决策边界**（decision&#10;        boundary），即算法对类别0和类别1的**分界线**。（决策边界可视化，**见书**）：随着邻居个数越来越多，决策边界也越来越**平滑**。更平滑的边界对应**更简单**的模型。换句话说，**使用更少的邻居对应更高的模型复杂度，而使用更多的邻居对应更低的模型复杂度**。&#10;&#10;        书本图2-7：&#10;&#10;         **邻居数越多，模型越简单，训练集精度下降，测试集精度大体先升后降。**&#10;&#10;    3.  **k近邻回归**&#10;&#10;        ...**//todo**&#10;&#10;    4.  分析KNeighborsRegressor&#10;&#10;        ...&#10;&#10;    5.  **优点、缺点和参数**&#10;&#10;        一般来说，KNeighbors分类器有**2个**重要参数：**邻居个数**与**数据点之间距离的度量方法**。&#10;&#10;        在实践中，使用**较小**的邻居个数（比如3个或5个）往往可以得到**比较好**的结果，但你应&#10;&#10;        该调节这个参数。**默认**使用**欧式距离**，它在&#10;&#10;        许多情况下的效果都**很好**。&#10;&#10;        k-NN的优点之一就是模型很**容易理解**，但如果训练集**很大**（特征数很多或者样本数很大），预测速度可能会**比较慢**。使用k-NN算法时，对数据进行**预处理**是**很重要**的（见第3章）。对于大多数特征的大多数取值都为0的数据集（所谓的**稀疏数据集**）来说，这一算法的效果**尤其不好**。（kNN在在实践中往往不会用到，下面的这种方法就没有这两个缺点）&#10;&#10;">
    </outline>
    <outline text="2.3.3 线性模型 Linear models" _note="线性模型利用输入特征的线性函数（linear function）进行预测&#10;&#10;1.  **用于回归的线性模型**&#10;&#10;    -   对于回归问题，线性模型预测的一般公式如下：&#10;&#10;        ***ŷ* = *w*\[0\] \* *x*\[0\] + *w*\[1\] \* *x*\[1\] + … +&#10;        *w*\[*p*\] \* *x*\[*p*\] + *b***&#10;&#10;    *x*\[0\] 到 *x*\[*p*\] 表示单个数据点的**特征**（本例中特征个数为&#10;    *p*+1），*w* 和 *b* 是学习模型的&#10;&#10;    **参数**，*ŷ* 是模型的**预测结果**&#10;&#10;    -   对于**单一**特征的数据集，公式如下：&#10;&#10;        ***ŷ* = *w*\[0\] \* *x*\[0\] + *b***&#10;&#10;        在一维 wave 数据集上学习参数 *w*\[0\] 和 *b*：&#10;&#10;        ``` {.python}&#10;        In[25]:&#10;        mglearn.plots.plot_linear_regression_wave()&#10;        Out[25]:&#10;        w[0]: 0.393906 b: -0.031804&#10;        ```&#10;&#10;        用于回归的线性模型可以表示为这样的回归模型：对单一特征的预测结果是一条**直线**，两个特征时是一个**平面**，或者在更高维度（即更多特征）时是一个**超平面**。&#10;&#10;2.  **线性回归（又名普通最小二乘法）**（ordinary least&#10;    squares，OLS）,是**回归问题**最简单也最经&#10;&#10;    典的线性方法&#10;&#10;    -   线性回归寻找参数w和b，使得对训练集的预测值与真实的回归目标值y之间的**均方误差最小**&#10;&#10;    -   **均方误差（mean squared&#10;        error）**是预测值与真实值之差的平方和除以样本数&#10;&#10;        $$(预测值-真实值)^2/样本数$$&#10;    -   线性回归没有参数，这是一个优点，但也因此无法控制模型的复杂度&#10;&#10;    -   线性模型对**wave数据集**的预测结果（书P36 37）&#10;&#10;        ``` {.python}&#10;        from sklearn.linear_model import LinearRegressionX, &#10;        y = mglearn.datasets.make_wave(n_samples=60)&#10;        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)&#10;        ...&#10;        #“斜率”参数（w，也叫作权重或系数）被保存在coef_属性中，而偏移或截距（b）被保存在intercept_属性中：&#10;        #intercept_属性是一个浮点数，而coef_属性是一个NumPy数组&#10;        ```&#10;&#10;        ``` {.python}&#10;        print(&quot;Training set score: {:.2f}&quot;.format(lr.score(X_train, y_train))) 0.67&#10;        print(&quot;Test set score: {:.2f}&quot;.format(lr.score(X_test, y_test))) 0.66&#10;        ```&#10;&#10;        训练集和测试集上的分数非常接近。这说明可能存在**欠拟合**，而不是过拟合,对于一维数据集来说，过拟合的风险很小&#10;&#10;    -   线性模型对**波士顿房价数据集(复杂模型)**的预测结果&#10;&#10;        ``` {.python}&#10;        X, y = mglearn.datasets.load_extended_boston()&#10;        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)&#10;        lr = LinearRegression().fit(X_train, y_train)&#10;        ```&#10;&#10;        ``` {.python}&#10;        print(&quot;Training set score: {:.2f}&quot;.format(lr.score(X_train, y_train))) 0.95&#10;        print(&quot;Test set score: {:.2f}&quot;.format(lr.score(X_test, y_test))) 0.61&#10;        ```&#10;&#10;        测试集预测比训练集**低**很多（**过拟合**），因此应该试图找到一个可以**控制复杂度**的模型。标准线性回归最常用的替代方法之一就是**岭回归（ridge&#10;        regression）**&#10;&#10;3.  **岭回归**&#10;&#10;    -   预测公式与**普通最小二乘法**相同&#10;&#10;    -   对系数（w）的选择不仅要在训练数据上得到好的预测结果，而且还要拟合附加约束&#10;&#10;    -   我们还希望**系数尽量小**。换句话说，w的所有元素都应接近于0。直观上来看，这意味着每个特征对输出的影响应**尽可能小**（即斜率很小），**同时**仍给出很好的预测结果。这种**约束**是所谓**正则化（regularization）**的一个例子&#10;&#10;    -   正则化是指对模型做**显式约束**，以**避免过拟合**，岭回归用到的这种被称为**L2正则化**&#10;&#10;        ``` {.python}&#10;        from sklearn.linear_model import Ridge&#10;        ridge = Ridge().fit(X_train, y_train)&#10;        ```&#10;&#10;        ``` {.python}&#10;        print(&quot;Training set score: {:.2f}&quot;.format(ridge.score(X_train, y_train))) 0.89&#10;        print(&quot;Test set score: {:.2f}&quot;.format(ridge.score(X_test, y_test))) 0.75&#10;        ```&#10;&#10;        Ridge在训练集上的分数要低于LinearRegression，但在测试集上的分数更高。**复杂度更小**的模型意味着在**训练集**上的**性能更差**，但**泛化性能更好**&#10;&#10;    -   简单性和训练集性能二者对于模型的重要程度可以由用户通过设置**alpha**参数来指定&#10;&#10;    -   alpha的最佳设定取决于用到的**具体数据集**&#10;&#10;    -   **增大alpha**会使得**系数更加趋向于0**，从而降低训练集性能，但可能会**提高泛化性能**&#10;&#10;        ``` {.python}&#10;        ridge10 = Ridge(alpha=10).fit(X_train, y_train)&#10;        ```&#10;&#10;    -   **减小alpha**可以让系数受到的**限制更小**&#10;&#10;        ``` {.python}&#10;        ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)&#10;        ```&#10;&#10;        从数学的观点来看，Ridge**惩罚**了系数的L2范数或w的欧式长度&#10;&#10;    -   还有一种方法可以用来理解**正则化的影响**，就是固定alpha值，但**改变训练数据量**，(例:对波士顿房价数据集做**二次抽样**,...将模型性能作为数据集大小的函数进行绘图，这样的图像叫作**学习曲线**&#10;&#10;    -   //TODO...&#10;&#10;">
    </outline>
    <outline text="2.3.4 朴素贝叶斯分类器 Naive Bayes Classifiers" _note="-   训练速度更快，泛化能力比线性分类器稍差&#10;&#10;-   朴素贝叶斯模型如此高效的原因在于，它通过**单独**查看**每个特征**来学习参数，并从每个特征中收集简单的类别统计数据&#10;&#10;-   scikit-learn&#10;    中实现了三种朴素贝叶斯分类器：GaussianNB(**高斯**:任意连续数据)、BernoulliNB&#10;    (**伯努利**:二分类)和&#10;    MultinomialNB(**多项式**:假定输入数据为计数数据(每个特征代表某个对象的整数计数))&#10;&#10;    -   BernoulliNB 分类器计算每个特征不为0的元素个数，代码见书P53&#10;&#10;    -   MultinomialNB&#10;&#10;        计算每个类别中每个特征的平均值&#10;&#10;    -   GaussianNB&#10;&#10;        保存每个类别中每个特征的平均值和标准差&#10;&#10;    -   **优点、缺点和参数**&#10;&#10;        -   MultinomialNB和BernoulliNB都只有一个参数**alpha**，用于控制模型复杂度。可以将统计数据“平滑化”（smoothing）。alpha越大，平滑化越强，模型复杂度就越低。&#10;&#10;        -   GaussianNB主要用于高维数据，而另外两种朴素贝叶斯模型则广泛用于稀疏计数数据，比如文本。MultinomialNB的性能通常要优于BernoulliNB，特别是在包含很多非零特征的数据集（即大型文档）上&#10;&#10;        -   该模型对高维稀疏数据的效果很好，对参数的鲁棒性也相对较好。朴素贝叶斯模型是很好的基准模型，常用于非常大的数据集&#10;&#10;-   补充：&#10;&#10;    **贝叶斯公式**&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\贝叶斯公式.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\Bayes_Training_data.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\bayes_solution.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;">
    </outline>
    <outline text="2.3.5 决策树 Decision trees" _note="-   决策树是广泛用于**分类**和**回归**任务的模型&#10;&#10;-   **构造决策树**&#10;&#10;    对数据反复进行**递归划分**，直到划分后的每个区域（决策树的每个**叶结点**）只包含单一目标值（单一类别或单一回归值）。如果树中某个叶结点所包含数据点的目标值都相同，那么这个叶结点就是纯的（pure）&#10;&#10;    举例：**two\_moons数据集**&#10;&#10;-   **控制决策树的复杂度**&#10;&#10;    -   通常来说，构造决策树直到所有叶结点都是**纯的叶结点**，这会导致模型**非常复杂**，并且对训练数据**高度过拟合**。纯叶结点的存在说明这棵树在**训练集**上的精度是&#10;        **100%**&#10;&#10;        &lt;img src=&quot;.\ML_COURSE\Decision_Tree_1.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;        上图可以看出过拟合，在所有属于类别0的点中间有一块属于类别1的区域。另一方面，有一小条属于类别0的区域，包围着最右侧属于类别0的那个点。这并不是想象中决策边界的样子，这个**决策边界**过于关注**远离**同类别其他点的单个异常点&#10;&#10;    -   **防止过拟合**的**两种**常见策略&#10;&#10;        -   及早停止树的生长，也叫**预剪枝（pre-pruning）**（限制条件可能包括限制树的最大深度(max*depth)、限制叶结点的最大数目(max*leaf*nodes)，或者规定一个结点中数据点的最小数目(min*samples\_leaf)来防止继续划分）&#10;&#10;        -   先构造树，但随后删除或折叠信息量很少的结点，也叫**后剪枝（post-pruning）**或**剪枝（pruning）**&#10;&#10;        -   **scikit-learn**的决策树在DecisionTreeRegressor类和DecisionTreeClassifier类中实现。&#10;&#10;            scikit-learn**只实现了预剪枝**，没有实现后剪枝&#10;&#10;        -   在**乳腺癌(Breast\&#10;            Cancer)数据集**查看预剪枝的效果，固定树的random\_state（随机数种子：保持不变，则每次随机结果相同，方便对比）&#10;&#10;            **默认**展开树(纯叶子结点)&#10;&#10;            ``` {.python}&#10;            cancer = load_breast_cancer()&#10;            X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)&#10;            tree = DecisionTreeClassifier(random_state=0)tree.fit(X_train, y_train)&#10;            print(&quot;Accuracy on training set: {:.3f}&quot;.format(tree.score(X_train, y_train)))&#10;            print(&quot;Accuracy on test set: {:.3f}&quot;.format(tree.score(X_test, y_test)))&#10;            ```&#10;&#10;            ``` {.python}&#10;            Accuracy on training set: 1.000 &#10;            Accuracy on test set: 0.937&#10;            ```&#10;&#10;            **预剪枝**&#10;&#10;            ``` {.python}&#10;            tree = DecisionTreeClassifier(max_depth=4, random_state=0)&#10;            tree.fit(X_train, y_train)&#10;            ```&#10;&#10;            ``` {.python}&#10;            Accuracy on training set: 0.988&#10;            Accuracy on test set: 0.951&#10;            ```&#10;&#10;            **降低了训练集的精度，提高了测试集的精度**&#10;&#10;-   **分析决策树**&#10;&#10;    -   利用tree模块的export\_graphviz函数来将树可视化&#10;&#10;-   **树的特征重要性（feature importance）**&#10;&#10;    -   为每个特征对树的决策的重要性&#10;&#10;        进行排序，它都是一个介于0和1之间的数字&#10;&#10;    -   如果某个特征的feature*importance*很小，并**不能说明**这个特征没有提供任何信息，这只能说明该特征没有被树选中，可能是因为另一个特征也包含了同样的信息&#10;&#10;    -   与线性模型的系数不同，特征重要性始终为**正数**，也**不能说明**该特征对应哪个类别&#10;&#10;-   **优点、缺点和参数(Strengths, weaknesses and parameters)**&#10;&#10;    -   线性模型和回归树对RAM价格数据的预测结果对比&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\decision_tree VS linear_model prediction.png&quot; style=&quot;zoom: 50%;&quot; /&gt;&#10;&#10;&#10;    线性模型对测试数据(2000年以后)给出了很好的预测，不过忽略了训练数据和测试数据中一些更细微的变化。与之相反，树模型完美预测了**训练**数据，但是，一旦输入**超出**了模型训练数据的范围，模型就只能持续预测最后一个已知数据点，树不能在训练数据的范围之外生成“新的”响应。所有基于树的模型都有这个**缺点**&#10;&#10;    -   决策树有两个**优点**：&#10;&#10;        -   得到的模型很容易可视化，容易理解(non-experted)&#10;&#10;        -   算法完全不受**数据缩放**(data&#10;            scaling)的影响。由于**每个特征**被**单独处理**(processed&#10;            separately)，而且数据的划分也**不依赖于缩放**，因此决策树算法**不需要特征预处理**，比如归一化(normalization)或标准化(standardization)。**特别是**特征的**尺度**(scales)完全不一样时或者二元特征和连续特征**同时**(mix&#10;            of binary and continuous features)时，决策树的**效果很好**&#10;&#10;    -   决策树的**主要缺点**在于，即使做了预剪枝，它也经常会**过拟合**，**泛化性能很差**。因此，在大多数应用中，往往使用下面介绍的**集成方法**来替代单棵决策树&#10;&#10;">
    </outline>
    <outline text="2.3.6 决策树集成 Ensembles of Decision Trees" _note=" 都以决策树为基础**两种**集成模型：&#10;&#10; **随机森林（random forest）**和**梯度提升决策树（gradient boosted&#10;decision tree）**">
      <outline text="1.随机森林 random forest" _note="-   随机森林背后的**思想**(idea)是，每棵树的预测可能都相对较好，但可能对**部分数据过拟合**。&#10;&#10;    如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对&#10;&#10;&#10;这些树的结果**取平均值**来**降低过拟合**。既能**减少过拟合又能保持树的预测能力**&#10;&#10;-   随机森林中树的**随机化方法**有两种：&#10;&#10;    -   通过选择用于构造树的**数据点**&#10;&#10;    -   通过选择**每次划分测试的特征**(the features in each split test)&#10;&#10;-   **构造随机森林**&#10;&#10;    -   构造树的个数(**n\_estimators参数**)&#10;&#10;    -   算法对每棵树进行不同的**随机**选择,以确保树和树之间是有区别的(彼此独立)&#10;&#10;    -   想要构造一棵树，首先要对数据进行**自助采样（bootstrap&#10;        sample）**,从n*samples个数据点中**有放回地**（即同一样本可以被多次抽取）**重复随机抽取**一个样本，共抽取\*\*n*samples次\*\*，有些数据点会缺失(missing)或重复(repeated)&#10;&#10;    -   接下来，基于这个**新创建的数据集**来构造决策树，对之前介绍的决策树算法稍作修改：在每个结点处，算法**随机选择特征的一个子集**，并对**其中一个特征**寻找**最佳测试**，而**不是**对每个结点都寻找最佳测试。选择的特征个数由**max\_features**参数来控制&#10;&#10;    -   两种方法结合保证随机森林&#10;&#10;        -   **自助采样**：**每棵决策树**的**数据集都是略有不同**&#10;&#10;        -   每个结点的**特征选择**：**每棵树**中的**每次划分**都是**基于特征的不同子集**&#10;&#10;    -   如果max*features\*\*等于n*features**，那么每次划分都要考虑数据集的**所有特征**，在**特征选择**的过程中**没有**添加**随机性**（不过**自助采样依然存在随机性**）如果设置max\_features**等于1**，那么在划分时将无法选择对哪个特征进行测试，只能对随机选择的某个特征搜索不同的阈值。因此，如果max\_features**较大**，那么随机森林中的树将会**十分相似**(抽样和选择**重复率高**)，利用**最独特的特征**可以**轻松拟合数据**。如果max\_features**较小**，那么随机森林中的树将会**差异很大**，**为了**很好地**拟合**数据，每棵树的**深度都要很大\*\*&#10;&#10;    -   利用随机森林进行**预测**，算法首先对森林中的**每棵树**进行预测&#10;&#10;        -   对于**回归**问题：可以对这些结果**取平均值**作为最终预测&#10;&#10;        -   对于**分类**问题：用到了**“软投票”（soft&#10;            voting）**策略，每个算法做出“软”预测，给出每个可能的输出标签的**概率**。对所有树的预测概率**取平均值**，然后将**概率最大**的类别作为预测结果&#10;&#10;    -   **分析随机森林**&#10;&#10;        -   将由5棵树组成的随机森林应用到前面研究过的**two\_moons数据集**上：&#10;&#10;            ``` {.python}&#10;            X, y = make_moons(n_samples=100, noise=0.25, random_state=3)&#10;            X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,     random_state=42) #42&#10;            forest = RandomForestClassifier(n_estimators=5, random_state=2) #5棵树&#10;            forest.fit(X_train, y_train)&#10;            ```&#10;&#10;        -   每棵树学到的**决策边界**可视化，也将它们的总预测（即整个森林做出的预测）**可视化**：&#10;&#10;            &lt;img src=&quot;.\ML_COURSE\two_moons decision boundary.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;            5棵树学到的**决策边界大不相同**。每棵树都犯了一些错误，因为这里画出的一些训练点实际上**并没有包含**在这些树的训练集中，原因在于**自助采样**。随机森林比单独每一棵树的过拟合都要小，给出的决策边界也更符合直觉。用到**更多棵树**（通常是几百或上千），从而得到**更平滑**的边界&#10;&#10;        -   将包含**100棵树**的随机森林应用在**cancer数据集**上:&#10;&#10;            ``` {.python}&#10;            forest = RandomForestClassifier(n_estimators=100, random_state=0)&#10;            forest.fit(X_train, y_train)&#10;            print(&quot;Accuracy on training set: {:.3f}&quot;.format(forest.score(X_train, y_train)))&#10;            print(&quot;Accuracy on test set: {:.3f}&quot;.format(forest.score(X_test, y_test)))&#10;            ```&#10;&#10;            ``` {.python}&#10;            Accuracy on training set: 1.000&#10;            Accuracy on test set: 0.972&#10;            ```&#10;&#10;            在没有调节任何参数的情况下，精度比线性模型或单棵决策树都要**好**&#10;&#10;    -   类似地，随机森林也可以给出**特征重要性**（feature&#10;        importance），计算方法是将森林中**所有树**的**特征重要性**求和并**取平均**，随机森林给出的特征重要性要比单棵树给出的更为**可靠**：&#10;&#10;        图见P67&#10;&#10;        与单棵树相比，随机森林中有**更多**特征的重要性**不为零**&#10;&#10;    -   **优点、缺点和参数**&#10;&#10;        -   用于**回归和分类的随机森林**是目前应用**最广泛**的机器学习方法之一，**不需要**反复调节参数，**不需要**对数据进行缩放&#10;&#10;        -   在大型数据集上构建随机森林可能比较**费时间**，但在一台计算机的多个CPU内核上并行计算也很容易，可以用**n\_jobs参数**来调节使用的**内核个数**。使用更多的CPU内核，可以让**速度线性增加**（使用2个内核，随机森林的训练速度会**加倍**）&#10;&#10;        -   森林中的**树越多**，它对随机状态选择的**鲁棒性**就越好&#10;&#10;        -   对于**维度非常高**的**稀疏数据**（比如**文本数据**），随机森林的表现往往**不是很好**。对于这种&#10;&#10;            数据，使用**线性模型可能更合适**，对一个应用来说，如果时间和内存很重要的话，那么**换用**线性模型可能更为明智&#10;&#10;        -   调参：&#10;&#10;            -   n\_estimators总是**越大越好**，在你的时间/内存允许的情况下**尽量多**&#10;&#10;            -   max*features决定每棵树的随机性大小，较小的max*features可以**降低过拟合**,一般来说，好的经验就是使用**默认值**：对于**分类**，默认值是**max*features=sqrt(n*features)**；对于**回归**，默认值是**max*features=log2(n*features)**。增大max*features或max*leaf\_nodes有时也可以**提高**性能（improve&#10;                performance）。它还可以大大降低用于训练和预测的时间和空间要求&#10;&#10;">
      </outline>
      <outline text="2.梯度提升回归树（梯度提升机） gradient boosted regression tree (gradient boosting machines)" _note="-   这个模型既可以用于回归也可以用于分类&#10;&#10;-   与随机森林方法不同，梯度提升采用**连续**的方式构造树，每棵树都试图**纠正**前一棵树的错误。**默认**情况下，&#10;&#10;    梯度提升回归树中**没有随机化**，而是用到了**强预剪枝**&#10;&#10;-   梯度提升树通常使用**深度很小**（1到5之间）的树，这样模型占用的**内存更少**，预测速度也**更快**&#10;&#10;-   梯度提升背后的主要思想是合并许多简单的模型（在这个语境中叫作**弱学习器**），比如深&#10;&#10;    度较小的树。每棵树只能对部分数据做出好的预测，因此，添加的树越来越多，可以**不断**&#10;&#10;    **迭代提高性能**&#10;&#10;-   对参数设置更敏感，除了预剪枝与集成中树的数量之外，梯度提升的另一个重要参数是**learning\_rate（学习**&#10;&#10;    **率）**，用于**控制**每棵树**纠正前一棵树的错误的强度**。较高的学习率意味着每棵树都可以做出较强的修正，这样模型**更为复杂**。通过增大n\_estimators来向集成中添加更多树，也可以**增加模型复杂度**，因为模型有**更多机会**纠正训练集上的错误&#10;&#10;-   乳腺癌数据集上应用GradientBoostingClassifier的示例。默认使用100棵树，最大深度是3，学习率为0.1&#10;&#10;    ``` {.python}&#10;    gbrt = GradientBoostingClassifier(random_state=0)&#10;    gbrt.fit(X_train, y_train)&#10;    print(&quot;Accuracy on training set: {:.3f}&quot;.format(gbrt.score(X_train, y_train)))&#10;    print(&quot;Accuracy on test set: {:.3f}&quot;.format(gbrt.score(X_test, y_test)))&#10;    ```&#10;&#10;    ``` {.python}&#10;    Accuracy on training set: 1.000&#10;    Accuracy on test set: 0.958&#10;    ```&#10;&#10;    由于训练集精度达到100%，所以很可能存在过拟合。为了降低过拟合，我们可以**限制最大深度**来加强预剪枝，也可以**降低学习率**：&#10;&#10;    ``` {.python}&#10;    # max_depth=1&#10;    gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)&#10;    gbrt.fit(X_train, y_train)&#10;    ```&#10;&#10;    ``` {.python}&#10;    Accuracy on training set: 0.991 &#10;    Accuracy on test set: 0.972&#10;    ```&#10;&#10;    ``` {.python}&#10;    # learning_rate=0.01&#10;    gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)&#10;    gbrt.fit(X_train, y_train)&#10;    ```&#10;&#10;    ``` {.python}&#10;    Accuracy on training set: 0.988&#10;    Accuracy on test set: 0.965&#10;    ```&#10;&#10;    两种方法都降低了训练集精度，在这个例子中，减小树的最大深度**显著提升**了模型性能，而降低学习率仅稍稍提高了泛化性能&#10;&#10;-   将**特征重要性**可视化&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\feature importances gradient boosting cancer.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;    梯度提升树的特征重要性与随机森林的特征重要性有些类似，不过梯度提升**完全忽略了某些特征**&#10;&#10;-   由于梯度提升和随机森林两种方法在类似的数据上表现得都很好，因此一种常用的方法就是**先尝试随机森林**，它的**鲁棒性**很好。如果随机森林效果很好，但预测时间**太长**，或者机器学习模型**精度**小数点后第二位的**提高**也很重要，那么**切换成梯度提升**通常会有用&#10;&#10;-   **优点、缺点和参数**&#10;&#10;    -   其**主要缺点**是需要**仔细调参**，而且**训练时间**可能会**比较长**&#10;&#10;    -   这一算法**不需要**对数据进行缩放就可以表现得很好，而且**也适用**于二元特征与连续特征同时存在的数&#10;&#10;        据集。与其他基于树的模型相同，它也通常**不适用**于**高维稀疏数据**&#10;&#10;    -   **主要参数**包括**树的数量n\_estimators**和**学习率learning\_rate**（用于控制每棵树对前一棵树的错误的纠正强度）&#10;&#10;    -   这两个参数**高度相关**（highly&#10;        interconnected），因为learning\_rate**越低**，就需要**更多的树**来构建具有相似复杂度的模型&#10;&#10;    -   随机森林的n*estimators值总是越大越好，但梯度提升不同，增大n*estimators会导致模型更加复杂，进而可能导致**过拟合**，**通常**的做法是根据**时间和内存**的预算（budget）选择合适的n*estimators，然后对不同的learning*rate进行遍历&#10;&#10;    -   另一个**重要参数**是max*depth（或max*leaf*nodes），用于降低每棵树的复杂度。梯度提升模型的max*depth一般不超过5&#10;&#10;">
      </outline>
    </outline>
    <outline text="2.3.7 核支持向量机 Kernelized Support Vector Machines （SVM）" _note=" //TODO...&#10;&#10;1.  **线性模型与非线性特征**&#10;&#10;2.  **核技巧 （kernel trick）**&#10;&#10;    -   **原理**：直接计算扩展特征表示中数据点之间的距离（更准确地说是内积），而不用实际对扩展进行计算&#10;&#10;    -   对于支持向量机，将数据映射到更高维空间中有**两种**常用的方法：一种是**多项式核**，在一定阶数内计算原始特征所有可能的多项式（比如feature1&#10;        \*\* 2 \* feature2 \*\* 5）；另一种是径向基函数（radial basis&#10;        function，**RBF**）核，也叫**高斯核**&#10;&#10;3.  **理解SVM**&#10;&#10;    -   在训练过程中，SVM学习**每个训练数据点**对于**表示两个类别之间的决策边界**的**重要性**。通&#10;&#10;        常只有**一部分**训练数据点对于定义决策边界来说很重要：位于**类别之间边界**上的那些**点**。&#10;&#10;        这些点叫作**支持向量（support vector）**，支持向量机正是由此得名&#10;&#10;4.  **SVM调参**&#10;&#10;    -   将**RBF核SVM**应用到**乳腺癌数据集**上。默认情况下，C=1，gamma=1/n\_features：&#10;&#10;        ``` {.python}&#10;        X_train, X_test, y_train, y_test = train_test_split(    cancer.data, cancer.target, random_state=0)&#10;        svc = SVC()&#10;        svc.fit(X_train, y_train)&#10;        print(&quot;Accuracy on training set: {:.2f}&quot;.format(svc.score(X_train, y_train)))&#10;        print(&quot;Accuracy on test set: {:.2f}&quot;.format(svc.score(X_test, y_test)))&#10;        ```&#10;&#10;        ``` {.python}&#10;        Accuracy on training set: 1.00&#10;        Accuracy on test set: 0.63&#10;        ```&#10;&#10;        训练集上的分数十分完美，但在测试集上的精度只有63%，存在**相当严重的过拟合**&#10;&#10;    -   SVM对**参数的设定**和**数据的缩放**非常**敏感**。特别地，它要求所有特征有**相似的变化范围**&#10;&#10;        查看breast cancer数据集的特征范围：&#10;&#10;        &lt;img src=&quot;.\ML_COURSE\cancer magnitude.png&quot; style=&quot;zoom: 33%;&quot; /&gt;&#10;&#10;        乳腺癌数据集的特征具有**完全不同的数量级**，这对其他模型来说（比如**线性模型**）可能是**小问题**，但对**核SVM**却有**极大影响**&#10;&#10;5.  **为SVM预处理数据**&#10;&#10;    -   解决这个问题的一种方法就是对**每个特征**进行**缩放**，使其大致都位于同一范围（常用的缩放方法就是将所有特征缩放到0和1之间&#10;        **MinMaxScaler**）&#10;&#10;    -   手动实现MinMax缩放：&#10;&#10;        ``` {.python}&#10;        [IN]&#10;        # 计算训练集中每个特征的最小值&#10;        min_on_training = X_train.min(axis=0)&#10;        # 计算训练集中每个特征的范围（最大值-最小值）&#10;        range_on_training = (X_train - min_on_training).max(axis=0)&#10;        # 减去最小值，然后除以范围&#10;        # 这样每个特征都是min=0和max=1&#10;        X_train_scaled = (X_train - min_on_training) / range_on_training&#10;        print(&quot;Minimum for each feature\n{}&quot;.format(X_train_scaled.min(axis=0)))&#10;        print(&quot;Maximum for each feature\n {}&quot;.format(X_train_scaled.max(axis=0)))&#10;        ```&#10;&#10;        ``` {.python}&#10;        [OUT]&#10;        Minimum for each feature[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]&#10;        Maximum for each feature [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]&#10;        ```&#10;&#10;        ``` {.python}&#10;        [IN]&#10;        # 利用训练集的最小值和范围对测试集做相同的变换（详见第3章）&#10;        X_test_scaled = (X_test - min_on_training) / range_on_training&#10;        svc = SVC()&#10;        svc.fit(X_train_scaled, y_train) # X_train_scaled&#10;        print(&quot;Accuracy on training set: {:.3f}&quot;.format(svc.score(X_train_scaled, y_train)))&#10;        print(&quot;Accuracy on test set: {:.3f}&quot;.format(svc.score(X_test_scaled, y_test)))&#10;        ```&#10;&#10;        ``` {.python}&#10;        [OUT]&#10;        Accuracy on training set: 0.948 &#10;        Accuracy on test set: 0.951&#10;        ```&#10;&#10;        数据缩放的作用很大！实际上模型现在处于**欠拟合**的状态，因为训练集和测试集的**性能非常接近**，但还没有接近100%的精度。从这里开始，我们可以尝试**增大C**或**gamma**来**拟合**更为**复杂**的模型&#10;&#10;        ``` {.python}&#10;        svc = SVC(C=1000)&#10;        svc.fit(X_train_scaled, y_train)&#10;        ```&#10;&#10;        ``` {.python}&#10;        Accuracy on training set: 0.988&#10;        Accuracy on test set: 0.972	&#10;        ```&#10;&#10;        在这个例子中，增大C可以显著改进模型，得到97.2%的精度&#10;&#10;6.  **优点、缺点和参数**&#10;&#10;    -   它在低维数据和高维数据（即很少特征和很多特征）上的表现都很好，但对样本个数的**缩放表现不好**&#10;&#10;    -   SVM的另一个**缺点**是，**预处理数据**和**调参**都需要**非常小心**，所以很多应用都使用基于树的模型&#10;&#10;    -   此外，SVM模型**很难检查**（inspect），可能很难理解为什么会这么预测，而且也难以将模型向非专家进行解释&#10;&#10;    -   不过SVM仍然是值得尝试的，**特别**是所有特征的**测量单位相似**（比如都是**像素密度**）而且**范围也差不多**时&#10;&#10;    -   核SVM的**重要参数**是**正则化参数C**、**核的选择**以及与**核相关的参数**&#10;&#10;    -   这里主要讲的是**RBF核**，RBF核**只有一个**参数**gamma**，它是**高斯核宽度**的**倒数**&#10;&#10;    -   gamma和C控制的都是**模型复杂度**，**较大的值**都对应更为**复杂**的模型，因此，这两个参数的设定通常是**强烈相关**的，应该**同时调节**&#10;&#10;">
    </outline>
    <outline text="2.3.8 神经网络（深度学习）Neural Networks (Deep Learning)" _note="这里只讨论一些相对简单的方法，即用于分类和回归的**多层感知机**（multilayer&#10;perceptron，MLP），它可以作为研究更复杂的深度学习方法的起点。MLP也被称为（普通）**前馈神经网络**，有时也简称为**神经网络**">
      <outline text="1.神经网络模型" _note="-   MLP可以被视为**广义的线性模型**，执行**多层处理**后得到结论&#10;&#10;-   线性回归的预测公式:&#10;&#10;    *ŷ* = *w*\[0\] \* *x*\[0\] + *w*\[1\] \* *x*\[1\] + … +&#10;    *w*\[*p*\] \* *x*\[*p*\] + *b*&#10;&#10;    ŷ是输入特征x\[0\]到x\[p\]的加权求和，权重为**学到的系数**(coefficients)w\[0\]到w\[p\]。&#10;&#10;    可以将这个公式**可视化**:&#10;&#10;    ``` {.python}&#10;    display(mglearn.plots.plot_logistic_regression_graph())&#10;    ```&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\logistic regression visualization.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;-   在MLP中，**多次重复这个计算加权求和**的过程，首先计算代表**中间过程**的**隐单元（hidden**&#10;&#10;    **unit）**，然后**再计算这些隐单元的加权求和**并得到最终结果(**单隐层的多层感知机**)&#10;&#10;    ``` {.python}&#10;    display(mglearn.plots.plot_single_hidden_layer_graph())&#10;    ```&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\single hidden layer.png&quot; style=&quot;zoom: 50%;&quot; /&gt;&#10;&#10;    -   在计算完每个隐单元的加权求和之后，**对结果**再应用一个**非线性**函数(**激活函数**（activation&#10;        functios）)——通常是**校正非线性**（rectifying&#10;        nonlinearity，也叫校正线性单元或**relu**）或**正切双曲线**（tangens&#10;        hyperbolicus，**tanh**）。然后将这个函数的结果用于**加权求和**，计算得到输出ŷ&#10;&#10;    -   计算回归问题的ŷ的完整公式如下（使用tanh非线性）:&#10;&#10;        h\[0\] = tanh(w\[0, 0\] \* x\[0\] + w\[1, 0\] \* x\[1\] + w\[2,&#10;        0\] \* x\[2\] + w\[3, 0\] \* x\[3\] + b\[0\])&#10;&#10;        h\[1\] = tanh(w\[0, 0\] \* x\[0\] + w\[1, 0\] \* x\[1\] + w\[2,&#10;        0\] \* x\[2\] + w\[3, 0\] \* x\[3\] + b\[1\])&#10;&#10;        h\[2\] = tanh(w\[0, 0\] \* x\[0\] + w\[1, 0\] \* x\[1\] + w\[2,&#10;        0\] \* x\[2\] + w\[3, 0\] \* x\[3\] + b\[2\])&#10;&#10;        ŷ = v\[0\] \* h\[0\] + v\[1\] \* h\[1\] + v\[2\] \* h\[2\] + b&#10;&#10;        其中，**w**是**输入x与隐层h**之间的**权重**，**v**是**隐层h与输出ŷ**&#10;        之间的**权重**。权重w和v要&#10;&#10;        从数据中**学习得到**，x是输入特征，ŷ&#10;        是计算得到的输出，h是计算的中间结果&#10;&#10;-   **多隐层的多层感知机--&amp;gt;大型神经网络--&amp;gt;深度学习**&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\多隐层MLP.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;">
      </outline>
      <outline text="2.神经网络调参" _note=" MLPClassifier应用到**two\_moons数据集**&#10;&#10;``` {.python}&#10;from sklearn.neural_network import MLPClassifier&#10;from sklearn.datasets import make_moons&#10;X, y = make_moons(n_samples=100, noise=0.25, random_state=3)&#10;X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)&#10;mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train) #solver='lbfgs'&#10;...&#10;plt.xlabel(&quot;Feature 0&quot;)&#10;plt.ylabel(&quot;Feature 1&quot;)&#10;```&#10;&#10;&lt;img src=&quot;.\ML_COURSE\MLP two_moons.png&quot; style=&quot;zoom: 33%;&quot; /&gt;&#10;&#10;-   包含**100个隐单元**的神经网络在two\_moons数据集上学到的**决策边界**&#10;&#10;    神经网络学到的决策边界完全是**非线性**(nonlinear)的，但**相对平滑**(relatively&#10;    smooth)&#10;&#10;    **默认**情况下，MLP使用100个隐结点，**默认**的非线性激活函数时**relu**，如果想得到**更加平滑**（模型更复杂）的决策边界，可以**添加更多的隐单元**、**添加第二个隐层**或者使**用tanh&#10;    非线性**&#10;&#10;-   减少hidden units数量（从而降低了模型复杂度）&#10;&#10;    ``` {.python}&#10;    # 使用2个隐层，每个包含10个单元，这次使用tanh非线性&#10;    mlp = MLPClassifier(solver='lbfgs', activation='tanh', random_state=0, hidden_layer_sizes=[10, 10])&#10;    mlp.fit(X_train, y_train)&#10;    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)&#10;    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)&#10;    plt.xlabel(&quot;Feature 0&quot;)&#10;    plt.ylabel(&quot;Feature 1&quot;)&#10;    ```&#10;&#10;&#10;    包含2个隐层、每个隐层包含10个隐单元的神经网络学到的决策边界（激活函数为**relu**）&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\relu.png&quot; style=&quot;zoom: 33%;&quot; /&gt;&#10;&#10;&#10;    包含2个隐层、每个隐层包含10个隐单元的神经网络学到的决策边界（激活函数为**tanh**）&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\tanh.png&quot; style=&quot;zoom: 33%;&quot; /&gt;&#10;&#10;-   最后，我们还可以利用**L2惩罚**使**权重趋向于0**，从而控制神经网络的**复杂度**，正如我们在&#10;&#10;    **岭回归**和**线性分类器**中所做的那样，MLPClassifier中调节L2惩罚的参数是**alpha**（与线&#10;&#10;    性回归模型中的**相同**），它的**默认值很小**（**弱正则化**）&#10;&#10;    **不同隐单元**个数与**alpha参数**的**不同**设定下的决策函数：&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\alpha n_hidden .png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;-   神经网络的一个重要性质是，在开始学习之前其权重是**随机**设置的，这种随机初始化（随机种子不同）会**影响**（对于较小的网络）学到的模型&#10;&#10;-   将MLPClassifier应用在乳腺癌数据集上&#10;&#10;    ``` {.python}&#10;    mlp = MLPClassifier(random_state=42)&#10;    mlp.fit(X_train, y_train)&#10;    ...&#10;    ```&#10;&#10;    ``` {.python}&#10;    Accuracy on training set: 0.92 &#10;    Accuracy on test set: 0.90&#10;    ```&#10;&#10;    -   MLP的精度相当好，但没有其他模型好。与较早的SVC例子相同，原因可能在于数据的**缩放**&#10;&#10;    -   **神经网络**也要求所有输入特征的**变化范围相似**，最**理想**的情况是**均值(mean)为0**、**方差(variance)为1**&#10;&#10;    -   此处手动完成缩放（第3章**StandardScaler**）&#10;&#10;        ``` {.python}&#10;        # 计算训练集中每个特征的平均值&#10;        mean_on_train = X_train.mean(axis=0)&#10;        # 计算训练集中每个特征的标准差&#10;        std_on_train = X_train.std(axis=0)&#10;        ```&#10;&#10;        ``` {.python}&#10;        # 减去平均值，然后乘以标准差的倒数(除以标准差)&#10;        # 如此运算之后，mean=0，std=1&#10;        X_train_scaled = (X_train - mean_on_train) / std_on_train&#10;        # 对测试集做相同的变换（使用训练集的平均值和标准差）&#10;        X_test_scaled = (X_test - mean_on_train) / std_on_train&#10;        ```&#10;&#10;        ``` {.python}&#10;        mlp = MLPClassifier(random_state=0)&#10;        mlp.fit(X_train_scaled, y_train)&#10;        ...&#10;        ```&#10;&#10;        ``` {.python}&#10;        Accuracy on training set: 0.991&#10;        Accuracy on test set: 0.965&#10;        ConvergenceWarning:&#10;            Stochastic Optimizer: Maximum iterations reached and the optimization hasn't 	 converged yet.&#10;        ```&#10;&#10;        模型给出了一个警告，告诉我们已经达到最大迭代次数,应该增加迭代次数：&#10;&#10;        ``` {.python}&#10;        mlp = MLPClassifier(max_iter=1000, random_state=0) # max_iter=1000&#10;        mlp.fit(X_train_scaled, y_train)&#10;        ...&#10;        ```&#10;&#10;        ``` {.python}&#10;        Accuracy on training set: 0.995&#10;        Accuracy on test set: 0.965&#10;        ```&#10;&#10;        增加迭代次数仅**提高了训练集性能**，但**没有提高泛化性能**&#10;&#10;        由于训练性能和测试性能之间仍有一些差距，所以我们可以尝试**降低模型复杂度**来得到更**好**&#10;&#10;        **的泛化性能**。这里我们选择**增大alpha参数**（变化范围**相当大**，从**0.0001到1**），以此向&#10;&#10;        **权重**添加**更强的正则化**：&#10;&#10;        ``` {.python}&#10;        mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)&#10;        mlp.fit(X_train_scaled, y_train)&#10;        ...&#10;        ```&#10;&#10;        ``` {.python}&#10;        Accuracy on training set: 0.988&#10;        Accuracy on test set: 0.972&#10;        ```&#10;&#10;        这得到了与我们**目前最好**的模型相同的性能&#10;&#10;        虽然可以分析神经网络学到了什么，但这通常比分析线性模型或基于树的模型**更为复杂**&#10;&#10;        观察神经网络在乳腺癌数据集上学到的第一个隐层权重的**热图**(heat&#10;        map):&#10;&#10;        &lt;img src=&quot;.\ML_COURSE\heat map neural network.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;">
      </outline>
      <outline text="3.优点、缺点和参数" _note="-   在机器学习的许多应用中，神经网络再次成为**最先进**的模型。它的**主要优点之一**是能够获&#10;&#10;    取大量数据中包含的信息，并构建**无比复杂**的模型。给定**足够的计算时间和数据**，并且**仔**&#10;&#10;    **细调节（tuning）参数**，神经网络**通常可以打败**其他机器学习算法（**无论**是**分类任务**还是**回归任务**）&#10;&#10;-   **缺点**:神经网络——特别是功能强大的大型神经网络——通常需要**很长的训练时间**,还需要仔细地**预处理**数据，&#10;&#10;    神经网络**调参**本身也是一门艺术（Tuning neural network parameters&#10;    is also an art onto itself）&#10;&#10;-   神经网络调参的**常用方法**是，首先创建一个**大到足以过拟合**的网络，确保这个网络可以对任务进行学习。知道训练数据可以被学习之后，要么**缩小(shrink)网络**，要么**增大alpha**来**增强正则化**，这可以**提高泛化性能**&#10;&#10;-   **solver参数**设定：&#10;&#10;    -   默认选项是**'adam'**，对数据的**缩放**相当**敏感**，始终将数据缩放为均值为0、方差为1（unit&#10;        variance）是很重要的&#10;&#10;    -   另一个选项是**'l-bfgs'**，其**鲁棒性**相当好，但在大型模型或大型数据集上的**时间会比较长**&#10;&#10;    -   更高级的**'sgd'**选项，许多深度学习研究人员都会用到&#10;&#10;    -   使用MLP时，建议使用'adam'和'l-bfgs'&#10;&#10;">
      </outline>
    </outline>
  </outline>
  <outline text="2.4 分类器的不确定度估计 Uncertainty Estimates from Classifiers" _note="scikit-learn接口的另一个有用之处，就是分类器能够给出预测的**不确定度估计**,不仅是分类器会预测一个测试点属于哪个类别，还包括它对这个预测的**置信程度**,scikit-learn中有两个函数可用于获取分类器的不确定度估计：**decision\_function**（决策函数）和**predict\_proba**（预测概率）&#10;&#10;-   构建一个GradientBoostingClassifier分类器（同时拥有decision*function和predict*proba两个方法）&#10;&#10;//TODO...">
    <outline text="2.4.1 决策函数 Decision Function">
    </outline>
    <outline text="2.4.2 预测概率 Predicting Probabilities" _note="-   predict*proba的输出是每个类别的概率，通常比decision*function的输出更容易理解。&#10;&#10;-   **过拟合更强**的模型可能会做出**置信程度更高**的预测，**即使**可能是**错**的。**复杂度越低**的模型通常对预测的**不确定度越大**。如果模型给出的不确定度**符合实际情况**，那么这个模型被称为**校正（calibrated）模型**。在校正模型&#10;&#10;    中，如果预测有70%的确定度，那么它在70%的情况下正确&#10;&#10;">
    </outline>
    <outline text="2.4.3 多分类问题的不确定度 Uncertainty in Multiclass Classification" _note="-   将这两个函数应用于**鸢尾花（Iris）数据集**，这是一个**三分类**数据集&#10;&#10;-   //TODO...&#10;&#10;">
    </outline>
  </outline>
  <outline text="2.5 小结与展望 Summary and Outlook" _note="关于何时使用哪种模型，下面是一份**快速总结**:">
    <outline text="快速总结 （由简单到复杂的模型）">
      <outline text="#1 最近邻 Nearest neighbors" _note=" 适用于**小型数据集**，是很好的基准（baseline）模型，很容易解释">
      </outline>
      <outline text="#2 线性模型 Linear models" _note="&#10;非常可靠的**首选算法**，适用于**非常大的数据集**，也适用于**高维数据**。">
      </outline>
      <outline text="#3 朴素贝叶斯 Naive Bayes" _note="&#10;**只适用于分类问题**。比线性模型速度还快，适用于**非常大的数据集**和**高维数据**。**精度**通&#10;&#10; 常要**低于线性模型**（less accurate）">
      </outline>
      <outline text="#4 决策树 Decision trees" _note=" 速度很**快**，**不需要数据缩放**，可以**可视化**，很**容易解释**">
      </outline>
      <outline text="#5 随机森林 Random forests" _note="&#10;几乎总是**比单棵**决策树的表现要**好**，**鲁棒性很好**，非常**强大**。**不需要数据缩放**。**不适用**&#10;&#10; **于高维（high-dimensional）稀疏（sparse）数据**">
      </outline>
      <outline text="#6 梯度提升决策树 Gradient boosted decision trees" _note="&#10;**精度**通常**比随机森林略高**。与随机森林相比，**训练**速度**更慢**，但**预测**速度**更快**，需要的&#10;&#10; **内存也更少**。比随机森林需要**更多的参数调节**">
      </outline>
      <outline text="#7 支持向量机 Support vector machines" _note="&#10;对于**特征含义相似**的**中等**大小的**数据集**很强大。**需要数据缩放**，**对参数敏感**">
      </outline>
      <outline text="#8 神经网络 Neural networks" _note="&#10;可以构建**非常复杂**的模型，**特别**是对于**大型数据集**而言。对**数据缩放敏感**，对**参数选取敏感**。&#10;&#10; 大型网络需要**很长的训练时间**">
      </outline>
    </outline>
  </outline>
</outline>
<outline text="Chapter3 Unsupervised Learning and Preprocessing 无监督学习与预处理">
  <outline text="3.1 无监督学习的类型 Types of unsupervised learning" _note="本章将研究**两种**类型的无监督学习：**数据集变换**与**聚类**&#10;&#10;-   **无监督变换（unsupervised transformation）**&#10;&#10;    -   常见**应用**：**降维（dimensionality reduction）**&#10;&#10;    -   另一个**应用**是找到“构成”数据的各个组成部分（例子：对文本文档集合进行主题提取）&#10;&#10;-   **聚类算法（clustering algorithm）**&#10;&#10;    -   将数据划分成**不同的组**，每组包含**相似的物项**&#10;&#10;">
  </outline>
  <outline text="3.2 无监督学习的挑战 Challenges in unsupervised learning" _note="-   **主要挑战**就是评估算法是否学到了有用的东西，无监督学习算法一般用于**不包含任何标签**信息的数据，所以**不知道**正确的**输出**应该是什么。因此很难判断一个模型是否“表现很好”。&#10;&#10;-   通常来说，**评估无监督算法结果**的**唯一**方法就是**人工检查**（manually）&#10;&#10;-   无监督算法通常可用于**探索性**的目的，而不是作为大型自动化系统的一部分&#10;&#10;-   无监督算法的另一个**常见应用**是**作为监督算法**的**预处理步骤**&#10;&#10;">
  </outline>
  <outline text="3.3 预处理与缩放 Preprocessing and Scaling">
    <outline text="3.3.1 不同类型的预处理 Diferent kinds of preprocessing" _note="-   StandardScaler：确保每个特征的**平均值（mean）为0**、**方差(variance)为1**，使所有特征都位于同一量&#10;&#10;    级，但**不能保证**特征任何特定的**最大值**和**最小值**&#10;&#10;-   RobustScaler：工作原理与StandardScaler类似，使用**中位数**和**四分位数**。这样RobustScaler会忽略与其他点有很大不同的数据点（比如**测量误差**）。这些与众不同的数据点也叫**异常值（outlier）**&#10;&#10;-   MinMaxScaler：**移动**数据，使所有特征都刚好位于0到1之间&#10;&#10;-   Normalizer：用到一种完全不同的缩放方法，使得特征向量的**欧式长度等于1**。每个数据点的缩放比例都**不相同**（乘以其长度的倒数）。如果只有**数据的方向（或角度）**是**重要**的，而**特征向量的长度无关紧要**，那么通常会使用这种**归一化\***\*（据点投射到半径为**1**的圆上（对于更高维度的情况，是球面））。&#10;&#10;-   **对比图**&#10;&#10;    &lt;img src=&quot;.\ML_COURSE\different Scaler.png&quot; style=&quot;zoom: 50%;&quot; /&gt;&#10;&#10;">
    </outline>
    <outline text="3.3.2 应用数据变换 Applying data transformations" _note="-   将核**SVM**（SVC）应用在**cancer**数据集上，并使用**MinMaxScaler**来预处理数据&#10;&#10;    ``` {.python}&#10;    from sklearn.preprocessing import MinMaxScaler&#10;    scaler = MinMaxScaler() # MinMaxScaler&#10;    scaler.fit(X_train) #X_train&#10;    MinMaxScaler(copy=True, feature_range=(0, 1))&#10;    ```&#10;&#10;    ``` {.python}&#10;    # 变换数据&#10;    X_train_scaled = scaler.transform(X_train) #transform&#10;    # 在缩放之前和之后分别打印数据集属性&#10;    print(&quot;transformed shape: {}&quot;.format(X_train_scaled.shape))&#10;    print(&quot;per-feature minimum before scaling:\n {}&quot;.format(X_train.min(axis=0)))&#10;    print(&quot;per-feature maximum before scaling:\n {}&quot;.format(X_train.max(axis=0)))&#10;    print(&quot;per-feature minimum after scaling:\n {}&quot;.format(    X_train_scaled.min(axis=0)))&#10;    print(&quot;per-feature maximum after scaling:\n {}&quot;.format(    X_train_scaled.max(axis=0)))&#10;    ```&#10;&#10;    对**测试集**进行变换:&#10;&#10;    ``` {.python}&#10;    # 对测试数据进行变换&#10;    X_test_scaled = scaler.transform(X_test)&#10;    # 在缩放之后打印测试数据的属性&#10;    ...min&#10;    ...max&#10;    ```&#10;&#10;    &lt;img src=&quot;F:\Typora_md\ML_COURSE\negative_scaled.png&quot; style=&quot;zoom:50%;&quot; /&gt;&#10;&#10;    对测试集缩放后的最大值和最小值不是1和0，因为MinMaxScaler（以及其他所有缩放器）总是&#10;&#10;    对训练集和测试集应用**完全相同的变换**。也就是说，**transform**方法总是减去**训练集**的最小值，然后除以**训练集**的范围，而这两个值可能与测试集的最小值和范围并**不相同**&#10;&#10;">
    </outline>
    <outline text="3.3.3 对训练数据和测试数据进行相同的缩放 Scaling training and test data the same way" _note="">
    </outline>
    <outline text="3.3.4 预处理对监督学习的作用 The efect of preprocessing on supervised learning" _note="-   **观察使用MinMaxScaler对学习SVC的作用（缩放前后对比）**&#10;&#10;-   ``` {.python}&#10;    from sklearn.svm import SVC&#10;    X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,&#10;     random_state=0)&#10;    svm = SVC(C=100)&#10;    svm.fit(X_train, y_train)&#10;    print(svm.score(X_test, y_test))&#10;    ```&#10;&#10;    ``` {.python}&#10;    0.629370629371&#10;    ```&#10;&#10;-   ``` {.python}&#10;    # preprocessing using 0-1 scaling 使用0-1缩放进行预处理&#10;    scaler = MinMaxScaler()&#10;    scaler.fit(X_train)&#10;    X_train_scaled = scaler.transform(X_train)&#10;    X_test_scaled = scaler.transform(X_test)&#10;    # learning an SVM on the scaled training data 在缩放数据上学习SVM&#10;    134 | Chapter 3: Unsupervised Learning and Preprocessing&#10;    svm.fit(X_train_scaled, y_train)&#10;    # scoring on the scaled test set 在缩放的测试集上计算分数&#10;    svm.score(X_test_scaled, y_test)&#10;    ```&#10;&#10;    ``` {.python}&#10;    0.965034965034965&#10;    ```&#10;&#10;-   ``` {.python}&#10;    # preprocessing using zero mean and unit variance scaling 利用零均值和单位方差的缩放方法进行预处理&#10;    from sklearn.preprocessing import StandardScaler&#10;    scaler = StandardScaler()&#10;    scaler.fit(X_train)&#10;    X_train_scaled = scaler.transform(X_train)&#10;    X_test_scaled = scaler.transform(X_test)&#10;    # learning an SVM on the scaled training data&#10;    svm.fit(X_train_scaled, y_train)&#10;    # scoring on the scaled test set&#10;    svm.score(X_test_scaled, y_test)&#10;    ```&#10;&#10;    ``` {.python}&#10;    0.95804195804195802&#10;    ```&#10;&#10;">
    </outline>
  </outline>
  <outline text="3.4 降维、特征提取与流形学习 Dimensionality Reduction, Feature Extraction and Manifold Learning">
    <outline text="3.4.1 主成分分析 Principal Component Analysis (PCA)">
    </outline>
    <outline text="3.4.2 非负矩阵分解 Non-Negative Matrix Factorization (NMF)">
    </outline>
    <outline text="3.4.3 用t-SNE进行流形学习 Manifold learning with t-SNE">
    </outline>
  </outline>
  <outline text="3.5 聚类 Clustering">
    <outline text="3.5.1 k均值聚类 k-Means clustering">
    </outline>
    <outline text="3.5.2 凝聚聚类 Agglomerative Clustering">
    </outline>
    <outline text="3.5.3 DBSCAN">
    </outline>
    <outline text="3.5.4 聚类算法的对比与评估">
      <outline text="1.用真实值评估聚类 P147" _note="-   调整rand指数&#10;    （adjustedrandindex，ARI）和归一化互信息（normalizedmutualinformation，NMI）&#10;&#10;-   使用ARI来比较k均值、凝聚聚类和DBSCAN算法&#10;&#10;-   ``` {.python}&#10;    from sklearn.metrics.cluster import adjusted_rand_score &#10;    ...&#10;    ```&#10;&#10;">
      </outline>
      <outline text="2.在没有真实值的情况下评估聚类 P148" _note="-   轮廓系数（silhouette&#10;    coeffcient）轮廓分数计算一个簇的紧致度，其值越大越好，最高分数为1。虽然&#10;&#10;    紧致的簇很好，但紧致度不允许复杂的形状&#10;&#10;-   ``` {.python}&#10;    from sklearn.metrics.cluster import silhouette_score&#10;    ```&#10;&#10;">
      </outline>
    </outline>
    <outline text="3.5.5 聚类方法小结 Summary of Clustering Methods">
    </outline>
    <outline text="Test1 课堂练习" _note="``` {.python}&#10;1.将knn算法运用于iris数据集，比较MinMaxScaler前后的score变化&#10;2.将KMeans算法运用于iris数据集(n_clusters=3),利用样本真实类别评价聚类结果&#10;&#10;/ML/test1/class_test1.ipynb&#10;&#10;#1 &#10;#(1)&#10;from sklearn.datasets import load_iris&#10;from sklearn.model_selection import train_test_split&#10;from sklearn.neighbors import KNeighborsClassifier&#10;&#10;iris = load_iris()&#10;&#10;X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)&#10;&#10;knn = KNeighborsClassifier(n_neighbors=1)&#10;knn.fit(X_train, y_train)&#10;&#10;print(&quot;Test set accuracy: {:.2f}&quot;.format(knn.score(X_test, y_test)))&#10;&#10;#(2)&#10;from sklearn.preprocessing import MinMaxScaler&#10;scaler = MinMaxScaler()&#10;scaler.fit(X_train)&#10;X_train_scaled = scaler.transform(X_train)# MinMax缩放&#10;X_test_scaled = scaler.transform(X_test)&#10;# svm.fit(X_train_scaled, y_train)&#10;knn = KNeighborsClassifier(n_neighbors=1)&#10;knn.fit(X_train_scaled, y_train)&#10;print(&quot;Scaled test set accuracy: {:.2f}&quot;.format(knn.score(X_test_scaled, y_test)))&#10;# X_train scaled&#10;print(&quot;per-feature minimum before scaling:\n{}&quot;.format(X_train.min(axis=0))) &#10;print(&quot;per-feature maximum before scaling:\n{}&quot;.format(X_train.max(axis=0)))&#10;print(&quot;per-feature minimum after scaling:\n{}&quot;.format(X_train_scaled.min(axis=0))) &#10;print(&quot;per-feature maximum after scaling:\n{}&quot;.format(X_train_scaled.max(axis=0)))&#10;# X_test scaled (range changed not in [0,1])&#10;print(&quot;per-feature minimum before scaling:\n{}&quot;.format(X_test.min(axis=0))) &#10;print(&quot;per-feature maximum before scaling:\n{}&quot;.format(X_test.max(axis=0)))&#10;print(&quot;per-feature minimum after scaling:\n{}&quot;.format(X_test_scaled.min(axis=0))) &#10;print(&quot;per-feature maximum after scaling:\n{}&quot;.format(X_test_scaled.max(axis=0)))&#10;&#10;#2&#10;from sklearn.datasets import load_iris&#10;from sklearn.cluster import KMeans&#10;&#10;iris=load_iris()&#10;data = iris.get('data')&#10;&#10;kmeans = KMeans(n_clusters=3) &#10;kmeans.fit(data)&#10;&#10;label_pred=kmeans.labels_&#10;print(label_pred)&#10;#print(kmeans.predict(data))&#10;&#10;a=label_pred.tolist()&#10;b=a[:50]&#10;c=a[50:100]&#10;d=a[100:150]&#10;b1=max(max(b.count(0),b.count(1)),b.count(2))&#10;print(b1) # b&#10;c1=max(max(c.count(0),c.count(1)),c.count(2))&#10;print(c1) # c&#10;d1=max(max(d.count(0),d.count(1)),d.count(2))&#10;print(d1) # d&#10;print(&quot;accuracy: {:.2f}&quot;.format((b1+c1+d1)/150))&#10;```&#10;&#10;">
    </outline>
    <outline text="Test2 课堂练习" _note="``` {.python}&#10;1.使用凝聚聚类算法对鸢尾花数据集进行聚类，并画出对应的树状图。&#10;2.使用DBSCAN算法对原始鸢尾花数据集进行聚类，查看聚类结果。将鸢尾花数据集进行最小最大缩放后，再次使用DBSCAN算法聚类，并查看聚类结果。&#10;3.使用凝聚聚类算法对鸢尾花数据集进行聚类，分别使用adjusted_rand_score和silhouette_score指标对聚类结果进行评价。&#10;&#10;/ML/test2/class_test2.ipynb&#10;&#10;#1&#10;from sklearn.cluster import AgglomerativeClustering&#10;from scipy.cluster.hierarchy import dendrogram,ward&#10;from sklearn.datasets import load_iris&#10;from sklearn.model_selection import train_test_split&#10;import matplotlib.pyplot as plt&#10;&#10;iris=load_iris()&#10;data=iris.data&#10;target=iris.target&#10;&#10;linkage_array = ward(data)&#10;dendrogram(linkage_array)&#10;ax = plt.gca()&#10;bounds = ax.get_xbound()&#10;ax.plot(bounds, [7.25, 7.25], '--', c='k')&#10;ax.plot(bounds, [4, 4], '--', c='k')&#10;ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})&#10;ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})&#10;plt.xlabel(&quot;Sample index&quot;)&#10;plt.ylabel(&quot;Cluster distance&quot;)&#10;&#10;#2&#10;#(1)&#10;from sklearn.cluster import DBSCAN&#10;dbscan = DBSCAN()&#10;clusters = dbscan.fit_predict(data)&#10;print(&quot;Cluster memberships:\n{}&quot;.format(clusters))&#10;&#10;#(2)&#10;from sklearn.preprocessing import MinMaxScaler&#10;from sklearn.model_selection import train_test_split&#10;scaler = MinMaxScaler()&#10;scaler.fit(data)&#10;X_scaled=scaler.transform(data)&#10;# dbscan = DBSCAN(eps=0.4, min_samples=5)&#10;dbscan=DBSCAN()&#10;clusters = dbscan.fit_predict(X_scaled)&#10;print(&quot;Cluster memberships:\n{}&quot;.format(clusters))&#10;&#10;#3&#10;from sklearn.metrics.cluster import adjusted_rand_score&#10;from sklearn.metrics.cluster import silhouette_score&#10;agg = AgglomerativeClustering(linkage='average',n_clusters=3)#linkage='ward' &#10;assignment = agg.fit_predict(data)&#10;print(adjusted_rand_score(assignment,target)) #&#10;print(silhouette_score(data,assignment)) #&#10;```&#10;&#10;">
    </outline>
  </outline>
</outline>
<outline text="Chapter4 Representing Data and Engineering Features 数据表示和特征工程" _note="-   分类特征（categorical feature）&#10;&#10;-   离散特征（discrete feature）&#10;&#10;-   对于某个特定应用来说，如何找到最佳数据表示，这个问题被称为特征工程（feature&#10;&#10;    engineering）&#10;&#10;-   adult数据集&#10;&#10;">
  <outline text="4.1.1 One-Hot编码（虚拟变量）One-Hot-Encoding (Dummy variables)" _note="-   one-hot 编码（one-hot-encoding）或 N 取一编码（one-out-of-N&#10;    encoding），也叫虚拟变量（dummy variable）&#10;&#10;-   新特征取值为 0 和 1&#10;&#10;-   pandas库&#10;&#10;-   ``` {.python}&#10;    import pandas as pd&#10;    from IPython.display import display&#10;    # 文件中没有包含列名称的表头，因此我们传入header=None&#10;    # 然后在&quot;names&quot;中显式地提供列名称&#10;    data = pd.read_csv(&#10;     &quot;data/adult.data&quot;, header=None, index_col=False,&#10;     names=['age', 'workclass', 'fnlwgt', 'education', 'education-num',&#10;     'marital-status', 'occupation', 'relationship', 'race', 'gender',&#10;     'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',&#10;     'income'])&#10;    # 为了便于说明，我们只选了其中几列&#10;    data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',&#10;     'occupation', 'income']]&#10;    # IPython.display可以在Jupyter notebook中输出漂亮的格式&#10;    display(data.head())&#10;    ```&#10;&#10;-   1.  **检查字符串编码的分类数据 Checking string-encoded categorical&#10;        data**&#10;&#10;        ``` {.python}&#10;        print(data.gender.value_counts())&#10;        ```&#10;&#10;        -   get\_dummies函数(自动变换所有具有对象类型（比如字符串）的列或所有分类的列)&#10;&#10;        ``` {.python}&#10;        In[4]:&#10;        print(&quot;Original features:\n&quot;, list(data.columns), &quot;\n&quot;)&#10;        data_dummies = pd.get_dummies(data)&#10;        print(&quot;Features after get_dummies:\n&quot;, list(data_dummies.columns))&#10;        Out[4]:&#10;        Original features:&#10;         ['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation',&#10;         'income']&#10;        Features after get_dummies:&#10;        ['age', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov',&#10;         'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private',&#10;          ...&#10;         'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners',&#10;         ...&#10;         'occupation_ Tech-support', 'occupation_ Transport-moving',&#10;         'income_ &lt;=50K', 'income_ &gt;50K']&#10;        ```&#10;&#10;        -   **连续特征** age 和 hours-per-week 没有发生变化&#10;&#10;        -   使用 values 属性将 data\_dummies 数据框（DataFrame）转换为&#10;            **NumPy 数组**，&#10;&#10;            然后在其上训练一个机器学习模型。在训练模型之前，注意要把**目标变量**（现在被编码为&#10;&#10;            两个 income&#10;            列）从数据中分离出来。将输出变量或输出变量的一些导出属性包含在特征表&#10;&#10;            示中，这是构建监督机器学习模型时一个非常常见的**错误**。&#10;&#10;        -   提取包含特征的列，也就是从 age 到 occupation\_&#10;            Transport-moving&#10;&#10;            的所有列。这一范围包含所有特征，但不包含目标：&#10;&#10;            ``` {.python}&#10;            features = data_dummies.ix[:, 'age':'occupation_ Transport-moving']&#10;            # 提取NumPy数组&#10;            X = features.values&#10;            y = data_dummies['income_ &gt;50K'].values&#10;            print(&quot;X.shape: {} y.shape: {}&quot;.format(X.shape, y.shape))&#10;            Out[6]:&#10;            X.shape: (32561, 44) y.shape: (32561,)&#10;            ```&#10;&#10;            现在数据的表示方式可以被 **scikit-learn** 处理&#10;&#10;            ``` {.python}&#10;            from sklearn.linear_model import LogisticRegression&#10;            from sklearn.model_selection import train_test_split&#10;            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)&#10;            logreg = LogisticRegression()&#10;            logreg.fit(X_train, y_train)&#10;            print(&quot;Test score: {:.2f}&quot;.format(logreg.score(X_test, y_test)))&#10;            ```&#10;&#10;            ``` {.python}&#10;            Test score: 0.81&#10;            ```&#10;&#10;">
  </outline>
  <outline text="4.1.2 数字可以编码分类变量">
  </outline>
  <outline text="4.2 分箱、离散化、线性模型与树 Binning, Discretization, Linear Models and Trees" _note="">
  </outline>
</outline>
  </body>
</opml>
